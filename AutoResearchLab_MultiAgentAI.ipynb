{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai\n",
        "!pip install google-search-results\n",
        "!pip install langchain langchain_community\n",
        "!pip install langgraph faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k44UcuA5XkDQ",
        "outputId": "52082aae-a19a-4b5b-d97d-7d7beef5d9e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.74)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.4.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.9-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "97c4dd1e56654d6db667c133a9246b57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2025.8.3)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=2ed7cc734296f804059b7398ee90f9ca5c54189da7a3bf8e9d01e87a344e4448\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/42/3e/aeb691b02cb7175ec70e2da04b5658d4739d2b41e5f73cd06f\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain_community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.74)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (0.4.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.11.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.6.5-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, faiss-cpu, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed faiss-cpu-1.12.0 langgraph-0.6.5 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.0 ormsgpack-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lPF5skPmXZA9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict, Any, Optional\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from serpapi import GoogleSearch\n",
        "\n",
        "# ----- LangGraph -----\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict\n",
        "\n",
        "# ----- FAISS -----\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# -------------------------------\n",
        "# Critic Agent with Fact-Checking\n",
        "# -------------------------------\n",
        "class CriticAgent:\n",
        "    def __init__(self, google_api_key: str):\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            temperature=0,\n",
        "            google_api_key=google_api_key\n",
        "        )\n",
        "        self.prompt_template = PromptTemplate(\n",
        "            input_variables=[\"facts\"],\n",
        "            template=\"\"\"\n",
        "You are a fact-checking agent. Your task is to evaluate each fact for accuracy based on general knowledge and reputable information sources.\n",
        "\n",
        "Facts to review:\n",
        "{facts}\n",
        "\n",
        "For each fact, return a JSON list with:\n",
        "- fact: The original fact (keep it short).\n",
        "- source: The source URL (if provided with the fact).\n",
        "- reliability_score: Integer 0–5 (5 = very reliable, 0 = false/misleading).\n",
        "- reasoning: A brief 1–2 sentence explanation.\n",
        "\n",
        "Return ONLY valid JSON in the following format:\n",
        "[\n",
        "  {{ \"fact\": \"...\", \"source\": \"https://...\", \"reliability_score\": 5, \"reasoning\": \"...\" }},\n",
        "  ...\n",
        "]\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    def evaluate_facts(self, facts: List[Dict[str, Any]]):\n",
        "        facts_str = json.dumps(facts, indent=2)\n",
        "        chain = LLMChain(llm=self.llm, prompt=self.prompt_template)\n",
        "        result = chain.run(facts=facts_str)\n",
        "\n",
        "        try:\n",
        "            json_string = result.strip()\n",
        "            if json_string.startswith(\"```json\"):\n",
        "                json_string = json_string.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "            parsed = json.loads(json_string)\n",
        "            if not isinstance(parsed, list):\n",
        "                raise ValueError(\"CriticAgent output is not a JSON list.\")\n",
        "\n",
        "            # Drop facts with reliability_score < 3\n",
        "            filtered = [f for f in parsed if f.get(\"reliability_score\", 0) >= 3]\n",
        "            return filtered\n",
        "\n",
        "        except (json.JSONDecodeError, ValueError) as e:\n",
        "            return {\n",
        "                \"error\": \"Failed to parse JSON from CriticAgent\",\n",
        "                \"raw_output\": result,\n",
        "                \"details\": str(e)\n",
        "            }\n",
        "\n",
        "# -------------------------------\n",
        "# Research Agent\n",
        "# -------------------------------\n",
        "class ResearchAgent:\n",
        "    def __init__(self, serpapi_key: str, critic_agent: CriticAgent, google_api_key: str):\n",
        "        self.serpapi_key = serpapi_key\n",
        "        self.critic_agent = critic_agent\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            temperature=0,\n",
        "            google_api_key=google_api_key\n",
        "        )\n",
        "        self.prompt_template = PromptTemplate(\n",
        "            input_variables=[\"query\", \"search_results\"],\n",
        "            template=\"\"\"\n",
        "You are a Research Agent.\n",
        "User query: \"{query}\"\n",
        "\n",
        "Search results:\n",
        "{search_results}\n",
        "\n",
        "From these results, extract the 5 most important factual statements.\n",
        "Each fact should be short (max 25 words) and tied to its source link.\n",
        "\n",
        "Return strictly JSON:\n",
        "[\n",
        "  {{ \"fact\": \"string\", \"source\": \"string\" }},\n",
        "  ...\n",
        "]\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    def serpapi_search(self, query: str):\n",
        "        search = GoogleSearch({\n",
        "            \"q\": query,\n",
        "            \"api_key\": self.serpapi_key,\n",
        "            \"num\": 10\n",
        "        })\n",
        "        results = search.get_dict()\n",
        "        return results.get(\"organic_results\", [])\n",
        "\n",
        "    def get_domain_from_url(self, url: str) -> Optional[str]:\n",
        "        try:\n",
        "            return urlparse(url).netloc\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def clean_and_score_facts(self, facts, raw_results):\n",
        "        cleaned = []\n",
        "        seen = set()\n",
        "        result_rank_map = {res.get(\"link\"): res.get(\"position\") for res in raw_results if res.get(\"link\")}\n",
        "\n",
        "        trusted_domains = set()\n",
        "        for res in raw_results[:5]:\n",
        "            domain = self.get_domain_from_url(res.get(\"link\", \"\"))\n",
        "            if domain:\n",
        "                trusted_domains.add(domain)\n",
        "\n",
        "        for item in facts:\n",
        "            fact = item.get(\"fact\", \"\").strip()\n",
        "            source = item.get(\"source\", \"\").strip()\n",
        "            source_domain = self.get_domain_from_url(source)\n",
        "\n",
        "            if not fact or fact.lower() in seen:\n",
        "                continue\n",
        "            seen.add(fact.lower())\n",
        "\n",
        "            score = 0\n",
        "            if len(fact) > 20:\n",
        "                score += 1\n",
        "            if any(char.isdigit() for char in fact):\n",
        "                score += 1\n",
        "            if source_domain and source_domain in trusted_domains:\n",
        "                score += 1\n",
        "            rank = result_rank_map.get(source)\n",
        "            if rank:\n",
        "                if rank <= 3:\n",
        "                    score += 2\n",
        "                elif rank <= 7:\n",
        "                    score += 1\n",
        "            if score > 2:\n",
        "                cleaned.append({\n",
        "                    \"fact\": fact,\n",
        "                    \"source\": source,\n",
        "                    \"confidence_score\": score,\n",
        "                    \"rank\": rank\n",
        "                })\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "    def run(self, query: str):\n",
        "        raw_results = self.serpapi_search(query)\n",
        "        search_results_text = []\n",
        "        for res in raw_results:\n",
        "            title = res.get(\"title\", \"\")\n",
        "            snippet = res.get(\"snippet\", \"\")\n",
        "            link = res.get(\"link\", \"\")\n",
        "            search_results_text.append(f\"{title} — {snippet} (URL: {link})\")\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=self.prompt_template)\n",
        "        structured_facts_str = chain.run(query=query, search_results=\"\\n\".join(search_results_text))\n",
        "\n",
        "        try:\n",
        "            json_string = structured_facts_str.strip()\n",
        "            if json_string.startswith(\"```json\"):\n",
        "                json_string = json_string.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "            facts = json.loads(json_string)\n",
        "            if not isinstance(facts, list):\n",
        "                raise ValueError(\"LLM output is not a JSON list.\")\n",
        "            for item in facts:\n",
        "                if not isinstance(item, dict) or \"fact\" not in item or \"source\" not in item:\n",
        "                    raise ValueError(\"LLM output list items are not properly formatted.\")\n",
        "        except (json.JSONDecodeError, ValueError) as e:\n",
        "            return {\"error\": \"Failed to parse or validate JSON from LLM\", \"raw_output\": structured_facts_str, \"details\": str(e)}\n",
        "\n",
        "        high_value_facts = self.clean_and_score_facts(facts, raw_results)\n",
        "        validated_facts = self.critic_agent.evaluate_facts(high_value_facts)\n",
        "        return validated_facts\n",
        "\n",
        "# -------------------------------\n",
        "# Writer Agent\n",
        "# -------------------------------\n",
        "class WriterAgent:\n",
        "    def __init__(self, google_api_key: str):\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            temperature=0.5,\n",
        "            google_api_key=google_api_key\n",
        "        )\n",
        "        self.summary_prompt = PromptTemplate(\n",
        "            input_variables=[\"facts\"],\n",
        "            template=\"\"\"\n",
        "You are a Writer Agent.\n",
        "Turn the validated facts below into a clear, human-friendly executive summary for a general audience.\n",
        "Add a concise 'Key Takeaways' section with bullet points.\n",
        "\n",
        "Facts:\n",
        "{facts}\n",
        "\n",
        "Return strictly JSON in the format:\n",
        "{{\n",
        "  \"executive_summary\": \"string\",\n",
        "  \"key_takeaways\": [\"point1\", \"point2\", ...]\n",
        "}}\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    def generate_summary(self, facts: List[Dict[str, Any]]):\n",
        "        facts_str = json.dumps(facts, indent=2)\n",
        "        chain = LLMChain(llm=self.llm, prompt=self.summary_prompt)\n",
        "        result = chain.run(facts=facts_str)\n",
        "\n",
        "        try:\n",
        "            json_string = result.strip()\n",
        "            if json_string.startswith(\"```json\"):\n",
        "                json_string = json_string.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "            parsed = json.loads(json_string)\n",
        "            return parsed\n",
        "        except (json.JSONDecodeError, ValueError) as e:\n",
        "            return {\n",
        "                \"error\": \"Failed to parse JSON from WriterAgent\",\n",
        "                \"raw_output\": result,\n",
        "                \"details\": str(e)\n",
        "            }\n",
        "\n",
        "    def create_visualization(self, facts: List[Dict[str, Any]]) -> Optional[str]:\n",
        "        \"\"\"Create a simple bar chart of reliability scores per fact; returns data URI.\"\"\"\n",
        "        try:\n",
        "            labels = []\n",
        "            scores = []\n",
        "            for f in facts:\n",
        "                t = f.get(\"fact\", \"\")\n",
        "                labels.append(t[:50] + \"...\" if len(t) > 50 else t)\n",
        "                scores.append(f.get(\"reliability_score\", 0))\n",
        "\n",
        "            if not labels:\n",
        "                return None\n",
        "\n",
        "            plt.figure(figsize=(8, 4))\n",
        "            plt.barh(labels, scores)\n",
        "            plt.xlabel(\"Reliability Score\")\n",
        "            plt.title(\"Fact Reliability Overview\")\n",
        "            plt.tight_layout()\n",
        "\n",
        "            buf = BytesIO()\n",
        "            plt.savefig(buf, format=\"png\")\n",
        "            plt.close()\n",
        "            buf.seek(0)\n",
        "            img_base64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
        "            return f\"data:image/png;base64,{img_base64}\"\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "# -------------------------------\n",
        "# Planner Agent\n",
        "# -------------------------------\n",
        "class PlannerAgent:\n",
        "    def __init__(self, google_api_key: str):\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            temperature=0.7,\n",
        "            google_api_key=google_api_key\n",
        "        )\n",
        "        self.plan_prompt = PromptTemplate(\n",
        "            input_variables=[\"facts\", \"summary\"],\n",
        "            template=\"\"\"\n",
        "You are a Planner Agent.\n",
        "Based on the validated facts and the executive summary, identify:\n",
        "1. Gaps in the research\n",
        "2. Open questions\n",
        "3. Related subtopics worth exploring\n",
        "\n",
        "Prioritize these suggestions based on:\n",
        "- Relevance to the original query\n",
        "- Importance to understanding the topic\n",
        "- Feasibility of researching them further\n",
        "\n",
        "Return strictly JSON in the format:\n",
        "{{\n",
        "  \"gaps_identified\": [\"gap1\", \"gap2\", ...],\n",
        "  \"open_questions\": [\"question1\", \"question2\", ...],\n",
        "  \"recommended_next_steps\": [\n",
        "    {{ \"step\": \"string\", \"priority\": \"High|Medium|Low\" }}\n",
        "  ]\n",
        "}}\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    def suggest_next_steps(self, facts: List[Dict[str, Any]], summary: Dict[str, Any]):\n",
        "        facts_str = json.dumps(facts, indent=2)\n",
        "        summary_str = json.dumps(summary, indent=2)\n",
        "        chain = LLMChain(llm=self.llm, prompt=self.plan_prompt)\n",
        "        result = chain.run(facts=facts_str, summary=summary_str)\n",
        "\n",
        "        try:\n",
        "            json_string = result.strip()\n",
        "            if json_string.startswith(\"```json\"):\n",
        "                json_string = json_string.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "            parsed = json.loads(json_string)\n",
        "            return parsed\n",
        "        except (json.JSONDecodeError, ValueError) as e:\n",
        "            return {\n",
        "                \"error\": \"Failed to parse JSON from PlannerAgent\",\n",
        "                \"raw_output\": result,\n",
        "                \"details\": str(e)\n",
        "            }\n",
        "\n",
        "# ==========================================================\n",
        "#                  FAISS Persistence Helpers\n",
        "# ==========================================================\n",
        "FAISS_DIR = \"faiss_index\"\n",
        "\n",
        "def get_embeddings(google_api_key: str):\n",
        "    return GoogleGenerativeAIEmbeddings(\n",
        "        model=\"models/text-embedding-004\",\n",
        "        google_api_key=google_api_key\n",
        "    )\n",
        "\n",
        "def load_or_create_faiss(embeddings) -> FAISS:\n",
        "    if os.path.isdir(FAISS_DIR):\n",
        "        try:\n",
        "            return FAISS.load_local(FAISS_DIR, embeddings, allow_dangerous_deserialization=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    dummy_text = \"This is a dummy text.\"\n",
        "    dummy_embedding = embeddings.embed_query(dummy_text)\n",
        "    embedding_dimension = len(dummy_embedding)\n",
        "\n",
        "    from faiss import IndexFlatL2\n",
        "    index = IndexFlatL2(embedding_dimension)\n",
        "    from langchain_community.docstore import InMemoryDocstore\n",
        "    from uuid import uuid4\n",
        "    empty_docs = []\n",
        "    docstore = InMemoryDocstore({str(uuid4()): Document(page_content=\"\") for _ in range(0)})\n",
        "    index_to_docstore_id = {}\n",
        "\n",
        "    return FAISS(embeddings.embed_query, index, docstore, index_to_docstore_id)\n",
        "\n",
        "\n",
        "def persist_facts_to_faiss(vs: FAISS, facts: List[Dict[str, Any]]):\n",
        "    texts = []\n",
        "    metas = []\n",
        "    now = int(time.time())\n",
        "    for f in facts:\n",
        "        # Compose a retrievable text: fact + source\n",
        "        fact_text = f.get(\"fact\", \"\")\n",
        "        source = f.get(\"source\") or f.get(\"source_url\") or \"\"\n",
        "        full_text = f\"{fact_text}\\nSOURCE: {source}\"\n",
        "        texts.append(full_text)\n",
        "        metas.append({\n",
        "            \"source\": source,\n",
        "            \"reliability_score\": f.get(\"reliability_score\"),\n",
        "            \"confidence_score\": f.get(\"confidence_score\"),\n",
        "            \"rank\": f.get(\"rank\"),\n",
        "            \"timestamp\": now\n",
        "        })\n",
        "    if texts:\n",
        "        vs.add_texts(texts=texts, metadatas=metas)\n",
        "        vs.save_local(FAISS_DIR)\n",
        "\n",
        "def retrieve_similar_facts(vs: FAISS, query: str, k: int = 5):\n",
        "    if vs is None:\n",
        "        return []\n",
        "    docs = vs.similarity_search(query, k=k)\n",
        "    return [{\"content\": d.page_content, \"metadata\": d.metadata} for d in docs]\n",
        "\n",
        "# ==========================================================\n",
        "#                  LangGraph\n",
        "# ==========================================================\n",
        "class GraphState(TypedDict, total=False):\n",
        "    query: str\n",
        "    validated_facts: List[Dict[str, Any]]\n",
        "    executive_summary: Dict[str, Any]\n",
        "    visualization_data_uri: Optional[str]\n",
        "    plan: Dict[str, Any]\n",
        "    retrieved_context: List[Dict[str, Any]]\n",
        "\n",
        "def build_graph(researcher: ResearchAgent,\n",
        "                writer: WriterAgent,\n",
        "                planner: PlannerAgent,\n",
        "                embeddings,\n",
        "                ):\n",
        "\n",
        "    vectorstore = load_or_create_faiss(embeddings)\n",
        "\n",
        "    def node_research(state: GraphState) -> GraphState:\n",
        "        validated = researcher.run(state[\"query\"])\n",
        "        return {\"validated_facts\": validated}\n",
        "\n",
        "    def node_persist_and_retrieve(state: GraphState) -> GraphState:\n",
        "        facts = state.get(\"validated_facts\", [])\n",
        "        if isinstance(facts, list) and all(isinstance(f, dict) for f in facts):\n",
        "             persist_facts_to_faiss(vectorstore, facts)\n",
        "\n",
        "        retrieved = retrieve_similar_facts(vectorstore, state[\"query\"], k=5)\n",
        "        return {\"retrieved_context\": retrieved}\n",
        "\n",
        "\n",
        "    def node_write(state: GraphState) -> GraphState:\n",
        "        facts = state.get(\"validated_facts\", [])\n",
        "        summary = writer.generate_summary(facts)\n",
        "        viz = writer.create_visualization(facts)\n",
        "        return {\"executive_summary\": summary, \"visualization_data_uri\": viz}\n",
        "\n",
        "    def node_plan(state: GraphState) -> GraphState:\n",
        "        facts = state.get(\"validated_facts\", [])\n",
        "        summary = state.get(\"executive_summary\", {})\n",
        "        plan = planner.suggest_next_steps(facts, summary)\n",
        "        return {\"plan\": plan}\n",
        "\n",
        "    graph = StateGraph(GraphState)\n",
        "    graph.add_node(\"research\", node_research)\n",
        "    graph.add_node(\"persist_and_retrieve\", node_persist_and_retrieve)\n",
        "    graph.add_node(\"writer\", node_write)\n",
        "    graph.add_node(\"planner\", node_plan)\n",
        "\n",
        "    graph.add_edge(START, \"research\")\n",
        "    graph.add_edge(\"research\", \"persist_and_retrieve\")\n",
        "    graph.add_edge(\"persist_and_retrieve\", \"writer\")\n",
        "    graph.add_edge(\"writer\", \"planner\")\n",
        "    graph.add_edge(\"planner\", END)\n",
        "\n",
        "    return graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "#                        Runner\n",
        "# ==========================================================\n",
        "if __name__ == \"__main__\":\n",
        "    SERPAPI_KEY = \"SerpAPI-key\"\n",
        "    GOOGLE_API_KEY = \"GoogleAPI-key\"\n",
        "\n",
        "    # Agents\n",
        "    critic = CriticAgent(GOOGLE_API_KEY)\n",
        "    researcher = ResearchAgent(SERPAPI_KEY, critic, GOOGLE_API_KEY)\n",
        "    writer = WriterAgent(GOOGLE_API_KEY)\n",
        "    planner = PlannerAgent(GOOGLE_API_KEY)\n",
        "\n",
        "    # Embeddings (langchain[google-genai])\n",
        "    embeddings = get_embeddings(GOOGLE_API_KEY)\n",
        "\n",
        "    # Build & run graph\n",
        "    app = build_graph(researcher, writer, planner, embeddings)\n",
        "\n",
        "    initial_state: GraphState = {\n",
        "        \"query\": \"Summarize the Knowledge distillation from this link: http://arxiv.org/pdf/2504.05299\"\n",
        "        #\"query\": \"Knowledge Distillation VLM\"\n",
        "    }\n",
        "    final_state = app.invoke(initial_state)\n",
        "\n",
        "    print(\"\\n=== EXECUTIVE SUMMARY ===\")\n",
        "    es = final_state.get(\"executive_summary\", {})\n",
        "    print(es.get(\"executive_summary\", \"\"))\n",
        "    print(\"\\n=== KEY TAKEAWAYS ===\")\n",
        "    for t in es.get(\"key_takeaways\", []):\n",
        "        print(f\"- {t}\")\n",
        "\n",
        "    print(\"\\n=== NEXT RESEARCH PLAN ===\")\n",
        "    print(json.dumps(final_state.get(\"plan\", {}), indent=2))\n",
        "\n",
        "    if final_state.get(\"visualization_data_uri\"):\n",
        "        print(\"\\nVisualization (base64 preview):\")\n",
        "        print(final_state[\"visualization_data_uri\"][:120] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inbok3AzXiKu",
        "outputId": "dd0c926f-6c95-47a6-d969-588813cd42f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
            "/tmp/ipython-input-891063002.py:179: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=self.llm, prompt=self.prompt_template)\n",
            "/tmp/ipython-input-891063002.py:180: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  structured_facts_str = chain.run(query=query, search_results=\"\\n\".join(search_results_text))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EXECUTIVE SUMMARY ===\n",
            "Knowledge distillation is a powerful technique for improving and streamlining acoustic models.  It works by transferring the knowledge learned by a large ensemble of models (the 'teacher') into a smaller, more efficient single model (the 'student'). This process allows for significant reductions in model size without necessarily sacrificing performance.  Research shows that various knowledge categories, training methods, and teacher-student relationships are key factors influencing the success of this technique. While not always guaranteed to maintain performance perfectly, when implemented correctly, knowledge distillation offers a promising path to creating faster, more deployable acoustic models.\n",
            "\n",
            "=== KEY TAKEAWAYS ===\n",
            "- Knowledge distillation shrinks large acoustic models into smaller, faster ones.\n",
            "- It achieves this by transferring knowledge from a group of models to a single model.\n",
            "- Success depends on the type of knowledge transferred, training methods, and the relationship between the original and distilled models.\n",
            "- While performance isn't always perfectly preserved, it's a valuable technique for improving efficiency.\n",
            "\n",
            "=== NEXT RESEARCH PLAN ===\n",
            "{\n",
            "  \"gaps_identified\": [\n",
            "    \"Lack of longitudinal data on the effectiveness of the intervention.\",\n",
            "    \"Limited diversity in the sample population, potentially limiting generalizability of findings.\",\n",
            "    \"Absence of cost-effectiveness analysis of the implemented program.\",\n",
            "    \"Unclear long-term sustainability of the intervention after the initial funding period.\",\n",
            "    \"No exploration of unintended consequences or negative side effects of the intervention.\"\n",
            "  ],\n",
            "  \"open_questions\": [\n",
            "    \"What are the long-term impacts of the intervention on participants?\",\n",
            "    \"How can the intervention be adapted to better serve diverse populations?\",\n",
            "    \"What is the cost-benefit ratio of implementing this program on a larger scale?\",\n",
            "    \"How can the program's sustainability be ensured beyond the initial funding?\",\n",
            "    \"Are there any unintended negative consequences associated with the intervention?\",\n",
            "    \"What are the key factors contributing to the success or failure of the intervention?\",\n",
            "    \"How can the intervention be improved based on the findings of this research?\"\n",
            "  ],\n",
            "  \"recommended_next_steps\": [\n",
            "    {\n",
            "      \"step\": \"Conduct a longitudinal study to assess the long-term effectiveness of the intervention.\",\n",
            "      \"priority\": \"High\"\n",
            "    },\n",
            "    {\n",
            "      \"step\": \"Conduct a cost-effectiveness analysis to determine the economic viability of the intervention.\",\n",
            "      \"priority\": \"High\"\n",
            "    },\n",
            "    {\n",
            "      \"step\": \"Explore strategies to ensure the long-term sustainability of the intervention.\",\n",
            "      \"priority\": \"High\"\n",
            "    },\n",
            "    {\n",
            "      \"step\": \"Replicate the study with a more diverse sample population to improve generalizability.\",\n",
            "      \"priority\": \"Medium\"\n",
            "    },\n",
            "    {\n",
            "      \"step\": \"Conduct qualitative research to explore unintended consequences and participant experiences.\",\n",
            "      \"priority\": \"Medium\"\n",
            "    },\n",
            "    {\n",
            "      \"step\": \"Develop a detailed implementation plan that addresses potential challenges and barriers.\",\n",
            "      \"priority\": \"Medium\"\n",
            "    },\n",
            "    {\n",
            "      \"step\": \"Investigate the factors that contribute to the success or failure of similar interventions in other contexts.\",\n",
            "      \"priority\": \"Low\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "Visualization (base64 preview):\n",
            "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAyAAAAGQCAYAAABWJQQ0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLj...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "48gcb4r4X4h5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}